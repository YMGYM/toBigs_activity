{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 15기 2주차 Optimization 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
    "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns) # 그냥 transForm도 가능하다.\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13391604, 0.30289478, 0.00871304])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z +=  X[i] * parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p =  \\frac{1}{1 + e^{-X_i\\theta }}$\n",
    "\n",
    "- sigmoid 함수라고도 부른다.\n",
    "- 분류 문제를 해결하기 위해 주로 사용한다(0~1의 범위를 갖기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X, parameters)\n",
    "    p = 1 / ( 1 + np.exp(-z))    # 로지스틱 함수\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6209065113854655"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수 == `MSE` 라 한다.\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
    "## $l(p) = -\\Sigma\\left \\{y_iX_i\\theta - log( 1 + e^{X_i\\theta})\\right \\} $\n",
    "\n",
    "아래 내용에선 $ -\\Sigma\\left \\{y_i log( \\frac {p_i}{1 -p_i}) + log( 1 - p_i)\\right \\}$ 로 작성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X, parameters)\n",
    "    loss = (y * np.log((p) / (1-p))) + np.log(1-p) # 오차값에 로그를 씌운 값입니다.\n",
    "    return -np.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X, parameters)\n",
    "    loss = (y - y_hat) ** 2 # Squared error\n",
    "    return 0.5 * np.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X, y, parameters)\n",
    "    loss = (loss / n) #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.756138453160803"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)= -\\Sigma(y_i - \\theta^{T}X_i)X_{ij} = \\Sigma(\\theta^{T}X_i - y_i )X_{ij}$\n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= -\\Sigma(y_i - p_i)x_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X, parameters)\n",
    "        gradient = np.sum(y_hat - y) * X[j]\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = np.sum(p-y) * X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5201103577954568"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 2, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))] # 0으로 초기화했다.\n",
    "    \n",
    "    for i in range(len(X_set)): # == X_set.shape[0] 과 같다.\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X, y, parameters, j, model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37.88452662475613, -6.718267409663314, 22.62416666626021]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch => range 문을 위해 +1을 해줌.\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "### 설명\n",
    "`batch_idx` 는 `X_train` 과 `batch_size` 를 받는다.\n",
    "\n",
    "`X_train` 은 학습에 사용되는 데이터 행렬이며, `batch_size` 는 미니배치의 크기이다.\n",
    "\n",
    "함수 내부에서 `N`, `nb` 두개의 변수를 만드는데, 각각 데이터의 개수와, 배치로 나눴을 때 배치 덩어리의 개수이다.\n",
    "\n",
    "총 데이터의 인덱스 (`N`의 길이)를 `idx` 배열에 저장한다.\n",
    "\n",
    "그 후에 그것을 `batch_size` 단위로 잘라서 `idx_list` 에 저장한다.\n",
    "\n",
    "즉 위의 함수는, 전체 데이터로 미니배치 데이터를 만들고, 배치 안에 들어있는 데이터를 인덱스로 표시하여 반환해주는 함수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= (learning_rate / n) # 평균을 구하고 학습률을 곱함\n",
    "    \n",
    "    parameters -= gradients # 전체 그래디언트 업데이트\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13139041, 0.30334266, 0.00720476])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train)) # 갱신된 파라미터 반환함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: 전체 데이터를 한번 돌 때 1에포크 증가\n",
    "- num_epoch: 전체 에포크를 몇 번 반복할 것인지\n",
    "<br>\n",
    "\n",
    "BGD: \"Batch Gradient Descent\" : 학습 한 번에 모든 데이터셋 전체를 순회하며 기울기를 구한다.  \n",
    "SGD: \"Stochasitc Gradient Descent\" : 랜덤하게 데이터 몇 개(또는 한 개)를 뽑아서 기울기를 구한다.  \n",
    "MGD: \"Mini batch Gradient Descent\" : 미니배치를 사용해(데이터의 일부를) 기울기를 구한다. 미니배치 전체를 돌면 학습 한 번에 모든 데이터셋을 돌게 된다. BGD와 SGD의 장점을 합친 방법으로 볼 수 있다.  \n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요  \n",
    "batch_size=1 -> SGD  \n",
    "batch_size=k -> MGD  \n",
    "batch_size=whole -> BGD  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0]) # 데이터의 feature 개수이다.\n",
    "    # 성능의 공정한 평가를 위해서 시드 고정\n",
    "    np.random.seed(0)\n",
    "    parameters = np.random.rand(N) # 랜덤한 계수로 초기화했다.\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i # 모델에 따라 손실 함수가 달라져야 한다.\n",
    "    loss = 999 # 임의의 loss(엄청 큰 값)\n",
    "    batch_idx_list = batch_idx(X_train, batch_size) # 모델을 1배치에 해당하는 X행렬 원소의 인덱스를 넣어 놓은 행렬 생성\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, loss_function) # 배치의 그래디언트를 구한다.\n",
    "            parameters = step(parameters, gradients, learning_rate, batch_size) # 구한 그래디언트를 미분하여 행렬을 갱신한다.\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, batch_size) # 손실 함수를 이용하여 손실값을 구한다.\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                print(f\"Early Stopped at loss: {new_loss}\") # early stop 표기\n",
    "                break\n",
    "            loss = new_loss\n",
    "            \n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 하이퍼파라미터의 모델 간 성능 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.9685644360044335  params: [0.53290843 0.71000918 0.58745526]  gradients: [0.015905073327806236, 0.005180187730894569, 0.015308116334578011]\n",
      "epoch: 100  loss: 0.5256474897515783  params: [-0.47666549  0.6217811  -0.39145511]  gradients: [0.005425983334753183, -0.0021837480381650797, 0.005549362855984847]\n",
      "epoch: 200  loss: 0.4511977556397476  params: [-0.8114574   0.90018792 -0.80401959]  gradients: [0.0020268837935590654, -0.0029220251697490342, 0.0032960135079598634]\n",
      "epoch: 300  loss: 0.41483251242497177  params: [-0.95435314  1.17642119 -1.09386919]  gradients: [0.001023671830597672, -0.0025766021771631118, 0.0025900367127658754]\n",
      "epoch: 400  loss: 0.3908947331324743  params: [-1.03607738  1.4149229  -1.33024423]  gradients: [0.0006696510861670546, -0.002207117844355731, 0.0021672074067224985]\n",
      "epoch: 500  loss: 0.373758731823303  params: [-1.09459009  1.61985188 -1.53062856]  gradients: [0.0005207561756130711, -0.001905971373589264, 0.0018575913364437633]\n",
      "epoch: 600  loss: 0.3609524031277944  params: [-1.14242439  1.79791059 -1.70378249]  gradients: [0.00044363676609285394, -0.0016666304116916353, 0.0016174066171437502]\n",
      "epoch: 700  loss: 0.35108722526008257  params: [-1.18415726  1.95453054 -1.85551242]  gradients: [0.00039442237147361143, -0.0014744543080549087, 0.001425998332408756]\n",
      "epoch: 800  loss: 0.3433052818317768  params: [-1.22167263  2.09380084 -1.98999853]  gradients: [0.00035768924093040677, -0.0013176039672104726, 0.001270414414552942]\n",
      "epoch: 900  loss: 0.33704718378221427  params: [-1.25588086  2.2187964  -2.11034892]  gradients: [0.000327630417804714, -0.0011874893199081992, 0.0011417853323118018]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.2870103 ,  2.33078509, -2.21789111])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, learning_rate = 0.05, batch_size=len(X_train), tolerance = 0.00001, model=\"logistic\")\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.9685644360044335  params: [0.53290843 0.71000918 0.58745526]  gradients: [0.015905073327806236, 0.005180187730894569, 0.015308116334578011]\n",
      "epoch: 100  loss: 0.5256474897515783  params: [-0.47666549  0.6217811  -0.39145511]  gradients: [0.005425983334753183, -0.0021837480381650797, 0.005549362855984847]\n",
      "epoch: 200  loss: 0.4511977556397476  params: [-0.8114574   0.90018792 -0.80401959]  gradients: [0.0020268837935590654, -0.0029220251697490342, 0.0032960135079598634]\n",
      "epoch: 300  loss: 0.41483251242497177  params: [-0.95435314  1.17642119 -1.09386919]  gradients: [0.001023671830597672, -0.0025766021771631118, 0.0025900367127658754]\n",
      "epoch: 400  loss: 0.3908947331324743  params: [-1.03607738  1.4149229  -1.33024423]  gradients: [0.0006696510861670546, -0.002207117844355731, 0.0021672074067224985]\n",
      "epoch: 500  loss: 0.373758731823303  params: [-1.09459009  1.61985188 -1.53062856]  gradients: [0.0005207561756130711, -0.001905971373589264, 0.0018575913364437633]\n",
      "epoch: 600  loss: 0.3609524031277944  params: [-1.14242439  1.79791059 -1.70378249]  gradients: [0.00044363676609285394, -0.0016666304116916353, 0.0016174066171437502]\n",
      "epoch: 700  loss: 0.35108722526008257  params: [-1.18415726  1.95453054 -1.85551242]  gradients: [0.00039442237147361143, -0.0014744543080549087, 0.001425998332408756]\n",
      "epoch: 800  loss: 0.3433052818317768  params: [-1.22167263  2.09380084 -1.98999853]  gradients: [0.00035768924093040677, -0.0013176039672104726, 0.001270414414552942]\n",
      "epoch: 900  loss: 0.33704718378221427  params: [-1.25588086  2.2187964  -2.11034892]  gradients: [0.000327630417804714, -0.0011874893199081992, 0.0011417853323118018]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.2870103 ,  2.33078509, -2.21789111])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate = 0.05, batch_size=len(X_train), tolerance = 0.00001, model=\"logistic\")\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.5462287389452337  params: [0.40423489 0.66940135 0.46288441]  gradients: [0.011513694380674963, 0.01055556584070781, 0.01334110015487331]\n",
      "epoch: 100  loss: 0.09990426559964741  params: [-1.27160695  2.27052645 -2.16434608]  gradients: [0.0018475445848693322, 0.0024697939142438556, 0.003463942367615205]\n",
      "Early Stopped at loss: 0.3588002232980838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.4147301 ,  2.77741606, -2.64260998])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.05, batch_size=16, num_epoch = 1000, tolerance = 0.00001, model=\"logistic\")\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 하이퍼파라미터로 비교했을 때 MGD가 가장 적은 loss 를 보였다.\n",
    "\n",
    "이후 MGD의 하이퍼파라미터를 조정해가며 학습을 비교해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.7801054684429901  params: [0.47816848 0.64454434 0.53211835]  gradients: [0.011799936842640017, 0.008623766736598808, 0.01765607925473053]\n",
      "epoch: 100  loss: 0.4126957459228419  params: [-0.33559063 -0.16921476 -0.28164075]  gradients: [0.0026848342358628714, -0.00218887136654188, 0.004219010159612238]\n",
      "epoch: 200  loss: 0.4126850786311187  params: [-0.33570808 -0.16933221 -0.2817582 ]  gradients: [0.0026835397229278165, -0.0021904714670937936, 0.004217039337701465]\n",
      "epoch: 300  loss: 0.4126850766367315  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.002683539480792918, -0.0021904717663850824, 0.00421703896906922]\n",
      "epoch: 400  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.002683539480747618, -0.0021904717664410745, 0.0042170389690002535]\n",
      "epoch: 500  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.0026835394807476105, -0.0021904717664410823, 0.004217038969000245]\n",
      "epoch: 600  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.0026835394807476105, -0.0021904717664410823, 0.004217038969000245]\n",
      "epoch: 700  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.0026835394807476105, -0.0021904717664410823, 0.004217038969000245]\n",
      "epoch: 800  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.0026835394807476105, -0.0021904717664410823, 0.004217038969000245]\n",
      "epoch: 900  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.0026835394807476105, -0.0021904717664410823, 0.004217038969000245]\n"
     ]
    }
   ],
   "source": [
    "# 배치 사이즈 2배\n",
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.05, batch_size=32, num_epoch = 1000, tolerance = 0.00001, model=\"logistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 사이즈가 증가하니까, 오차가 증가하는 것으로 보였다.\n",
    "\n",
    "한 배치에 처리해야할 양의 데이터가 많아서 오차가 증가하는 것으로 직관적으로 이해할 수 있을 듯 하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.6111239638187248  params: [0.51916809 0.70558299 0.57420036]  gradients: [0.0024131831863511877, 0.0021924542416963916, 0.00275905382281014]\n",
      "Early Stopped at loss: 0.5068818576097247\n"
     ]
    }
   ],
   "source": [
    "# 학습률 0.01 에폭 수 증가\n",
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.01, batch_size=16, num_epoch = 2000, tolerance = 0.00001, model=\"logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.6196808273885228  params: [0.5339465  0.71035941 0.58844676]  gradients: [0.001213173594239148, 0.0011009561104164496, 0.0013848003370405557]\n",
      "epoch: 100  loss: 0.2197450345412278  params: [-0.43871472  0.60922392 -0.35396112]  gradients: [0.0005154215239430915, 0.000506471626629391, 0.0007247899434964056]\n",
      "epoch: 200  loss: 0.1622597912104849  params: [-0.78286222  0.86283891 -0.76097751]  gradients: [0.0003115285599354422, 0.000329968400980178, 0.00051818923327476]\n",
      "epoch: 300  loss: 0.14260389966062684  params: [-0.93301302  1.12630304 -1.04360308]  gradients: [0.00025121502184278924, 0.000283969879598398, 0.0004523278545567537]\n",
      "epoch: 400  loss: 0.1311948695206934  params: [-1.01794449  1.35745235 -1.27418874]  gradients: [0.0002260340099148344, 0.00026750906417607025, 0.00042013385838201146]\n",
      "epoch: 500  loss: 0.12306665341035632  params: [-1.07734973  1.55772055 -1.47050078]  gradients: [0.0002124552536762419, 0.0002599031154811892, 0.0003996226379199426]\n",
      "epoch: 600  loss: 0.11674331405248291  params: [-1.12503431  1.73266422 -1.64088579]  gradients: [0.00020363631799703293, 0.00025549375138175616, 0.00038448657597471694]\n",
      "epoch: 700  loss: 0.11159781219101297  params: [-1.16624151  1.88715174 -1.79075609]  gradients: [0.00019717558217556715, 0.00025244041939112924, 0.0003724173486574007]\n",
      "epoch: 800  loss: 0.10729580074739876  params: [-1.20316055  2.02495877 -1.92401594]  gradients: [0.00019209535617601027, 0.00025006998679203004, 0.00036238345431926887]\n",
      "epoch: 900  loss: 0.10363172590778777  params: [-1.23682639  2.1489697  -2.04359169]  gradients: [0.00018793652174936765, 0.00024810890275472495, 0.0003538368121188256]\n",
      "epoch: 1000  loss: 0.10046736056554884  params: [-1.26781064  2.26140708 -2.15172456]  gradients: [0.00018445063421964383, 0.000246431821814255, 0.0003464422374619894]\n",
      "epoch: 1100  loss: 0.09770417383114526  params: [-1.29648372  2.3640085  -2.2501613 ]  gradients: [0.0001814848965580356, 0.0002449724472634246, 0.00033997250912117877]\n",
      "epoch: 1200  loss: 0.09526908263837727  params: [-1.32311841  2.4581534  -2.34028527]  gradients: [0.0001789353749580241, 0.00024369000422922712, 0.00033426248929676347]\n",
      "epoch: 1300  loss: 0.09310630397288296  params: [-1.347933    2.54495316 -2.42320834]  gradients: [0.00017672620487334815, 0.0002425559796512174, 0.0003291865774273315]\n",
      "epoch: 1400  loss: 0.09117232209623266  params: [-1.37111048  2.62531541 -2.49983646]  gradients: [0.00017479939977273817, 0.00024154858306752009, 0.00032464633380035703]\n",
      "epoch: 1500  loss: 0.08943259627289729  params: [-1.39280811  2.69999058 -2.57091692]  gradients: [0.0001731093103619523, 0.00024065025900895412, 0.00032056299365867204]\n",
      "Early Stopped at loss: 0.35901627717203066\n",
      "epoch: 1600  loss: 0.35901627717203066  params: [-1.41299121  2.76984588 -2.63675625]  gradients: [6.079577612944108e-05, -0.00018626043059662479, -7.357866693462126e-05]\n"
     ]
    }
   ],
   "source": [
    "# 학습률 0.005 # 에폭 수 증가 3000\n",
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.005, batch_size=16, num_epoch = 3000, tolerance = 0.00001, model=\"logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.6281598628995398  params: [0.5485153  0.71509225 0.60247636]  gradients: [2.4391012815701843e-05, 2.2110256134171673e-05, 2.7797589124863234e-05]\n",
      "epoch: 100  loss: 0.6109706517013486  params: [0.51888534 0.70551661 0.57394411]  gradients: [2.4098806306849923e-05, 2.1899339927623642e-05, 2.7563453666256634e-05]\n",
      "epoch: 200  loss: 0.5942031062304017  params: [0.48963756 0.69621638 0.54575383]  gradients: [2.3799509698308466e-05, 2.168062827789173e-05, 2.7320455478470447e-05]\n",
      "epoch: 300  loss: 0.5778638166515099  params: [0.46077977 0.68720001 0.51791603]  gradients: [2.3493463770867718e-05, 2.1454259871610062e-05, 2.7068777920297084e-05]\n",
      "epoch: 400  loss: 0.5619585435815306  params: [0.43231945 0.67847568 0.49044096]  gradients: [2.3181055870008754e-05, 2.1220425450616452e-05, 2.680866392169228e-05]\n",
      "epoch: 500  loss: 0.5464921496014734  params: [0.4042637  0.67005121 0.4633385 ]  gradients: [2.2862718734379334e-05, 2.0979369103906815e-05, 2.654041684700171e-05]\n",
      "epoch: 600  loss: 0.5314685370897134  params: [0.37661922 0.66193402 0.43661815]  gradients: [2.2538928555402653e-05, 2.0731388728652243e-05, 2.6264400381232306e-05]\n",
      "epoch: 700  loss: 0.5168905939012249  params: [0.34939224 0.65413108 0.41028889]  gradients: [2.2210202252338398e-05, 2.047683557672773e-05, 2.5981037365108123e-05]\n",
      "epoch: 800  loss: 0.5027601483371342  params: [0.32258849 0.64664883 0.38435914]  gradients: [2.1877093970844226e-05, 2.0216112827315346e-05, 2.569080753429347e-05]\n",
      "epoch: 900  loss: 0.4890779347094396  params: [0.29621314 0.6394931  0.3588367 ]  gradients: [2.1540190838862613e-05, 1.9949673155636364e-05, 2.539424415276342e-05]\n",
      "epoch: 1000  loss: 0.4758435706077172  params: [0.27027077 0.63266913 0.33372865]  gradients: [2.120010804027849e-05, 1.9678015301593916e-05, 2.5091929568370785e-05]\n",
      "epoch: 1100  loss: 0.463055546722036  params: [0.24476535 0.62618144 0.30904133]  gradients: [2.0857483292853735e-05, 1.940167967846839e-05, 2.4784489758294414e-05]\n",
      "epoch: 1200  loss: 0.4507112297767768  params: [0.21970016 0.62003383 0.28478022]  gradients: [2.0512970840930752e-05, 1.9121243098822623e-05, 2.4472587971042323e-05]\n",
      "epoch: 1300  loss: 0.4388068787948651  params: [0.19507784 0.61422934 0.26094996]  gradients: [2.0167235093803493e-05, 1.8837312730228316e-05, 2.4156917607670793e-05]\n",
      "epoch: 1400  loss: 0.42733767455533933  params: [0.17090029 0.60877019 0.23755425]  gradients: [1.9820944056117175e-05, 1.8550519425051968e-05, 2.3838194515566565e-05]\n",
      "epoch: 1500  loss: 0.4162977617457427  params: [0.14716874 0.60365779 0.21459585]  gradients: [1.947476270608356e-05, 1.8261510594227394e-05, 2.3517148891494005e-05]\n",
      "epoch: 1600  loss: 0.40568030296207136  params: [0.12388367 0.59889274 0.19207654]  gradients: [1.9129346479974712e-05, 1.7970942812939576e-05, 2.3194517005063003e-05]\n",
      "epoch: 1700  loss: 0.3954775433902577  params: [0.10104487 0.59447476 0.16999714]  gradients: [1.8785335017034e-05, 1.7679474355239016e-05, 2.2871032958382343e-05]\n",
      "epoch: 1800  loss: 0.38568088473014533  params: [0.0786514  0.59040277 0.14835747]  gradients: [1.844334630785996e-05, 1.7387757854244317e-05, 2.254742069219926e-05]\n",
      "epoch: 1900  loss: 0.37628096670848793  params: [0.05670166 0.58667487 0.12715637]  gradients: [1.8103971372195593e-05, 1.7096433274937274e-05, 2.2224386433801394e-05]\n",
      "epoch: 2000  loss: 0.36726775438071174  params: [0.03519334 0.58328835 0.10639174]  gradients: [1.7767769570019033e-05, 1.6806121368460554e-05, 2.19026117585637e-05]\n",
      "epoch: 2100  loss: 0.35863062934671175  params: [0.01412353 0.58023976 0.08606058]  gradients: [1.7435264624313427e-05, 1.6517417751739584e-05, 2.158274740700925e-05]\n",
      "epoch: 2200  loss: 0.35035848300374234  params: [-0.00651132  0.57752492  0.06615897]  gradients: [1.7106941406489833e-05, 1.6230887726042085e-05, 2.1265407964737232e-05]\n",
      "epoch: 2300  loss: 0.34243981002517765  params: [-0.02671533  0.57513895  0.0466822 ]  gradients: [1.6783243507769116e-05, 1.594706191487723e-05, 2.095116747583993e-05]\n",
      "epoch: 2400  loss: 0.33486280037941996  params: [-0.04649315  0.57307638  0.02762476]  gradients: [1.6464571593406748e-05, 1.566643276757348e-05, 2.0640556023728493e-05]\n",
      "epoch: 2500  loss: 0.32761542837757035  params: [-0.06584991  0.57133113  0.00898045]  gradients: [1.615128251275278e-05, 1.5389451941958366e-05, 2.0334057278632025e-05]\n",
      "epoch: 2600  loss: 0.3206855374489045  params: [-0.08479121  0.56989664 -0.0092576 ]  gradients: [1.58436891177592e-05, 1.51165285494918e-05, 2.0032106980079503e-05]\n",
      "epoch: 2700  loss: 0.31406091957632964  params: [-0.10332304  0.56876586 -0.02709682]  gradients: [1.554206072631669e-05, 1.4848028220269316e-05, 1.97350922966192e-05]\n",
      "epoch: 2800  loss: 0.30772938856683985  params: [-0.12145174  0.56793138 -0.04454516]  gradients: [1.5246624155013888e-05, 1.4584272924354072e-05, 1.9443351984590888e-05]\n",
      "epoch: 2900  loss: 0.30167884657267896  params: [-0.13918398  0.56738541 -0.06161099]  gradients: [1.4957565238537602e-05, 1.4325541470307239e-05, 1.9157177253192684e-05]\n",
      "Early Stopped at loss: 0.6070994735926688\n"
     ]
    }
   ],
   "source": [
    "# 학습률 0.0001 # 에폭 수 증가 3000\n",
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.0001, batch_size=16, num_epoch = 3000, tolerance = 0.00001, model=\"logistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 많이 낮추지 않는 이상 드라마틱한 변화는 일어나지 않는 것을 확인할 수 있었다.\n",
    "\n",
    "학습률이 낮아지면 낮아질수록 loss 가 줄어드는 속도가 변화하는 것을 확인할 수 있었다.\n",
    "\n",
    "이 경우, 시간에 따라서 학습률을 변화시키는  `RMSProp` 이나 `Adam` 과 같은 옵티마이저가 필요할 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd) # 학습한 bgd를 가지고 예측 결과 산출\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38,  2],\n",
       "       [ 4,  6]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn) # 구한 값으로 비교\n",
    "print(\"accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)]) # 편향 1 추가\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35767412, 2.75736157])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.9986840948275538  params: [0.5497891  0.71587949]  gradients: [-7.561815494881488e-05, -5.935681988982057e-05]\n",
      "epoch: 100  loss: 0.9052565600347382  params: [0.6461226  0.78425197]  gradients: [-7.465034046656767e-05, -5.872174005410375e-05]\n",
      "epoch: 200  loss: 0.8211387891420969  params: [0.74016257 0.85143529]  gradients: [-7.377128836470326e-05, -5.815208962016526e-05]\n",
      "epoch: 300  loss: 0.7457107531898034  params: [0.83210157 0.91754075]  gradients: [-7.2975121808612e-05, -5.764297702409657e-05]\n",
      "epoch: 400  loss: 0.6784240107491304  params: [0.92212075 0.98267209]  gradients: [-7.225551075980899e-05, -5.718921990573648e-05]\n",
      "epoch: 500  loss: 0.6187955057145834  params: [1.01038891 1.04692501]  gradients: [-7.16060244566833e-05, -5.678562327426635e-05]\n",
      "epoch: 600  loss: 0.5664011394450111  params: [1.09706226 1.11038717]  gradients: [-7.10203725226204e-05, -5.64271657865428e-05]\n",
      "epoch: 700  loss: 0.520869465471667  params: [1.18228451 1.17313832]  gradients: [-7.049255958595792e-05, -5.6109115331422514e-05]\n",
      "epoch: 800  loss: 0.48187572200273404  params: [1.26618736 1.23525077]  gradients: [-7.001697574036898e-05, -5.5827092773425836e-05]\n",
      "epoch: 900  loss: 0.4491363212454639  params: [1.34889107 1.29678978]  gradients: [-6.958844127662364e-05, -5.557709928674589e-05]\n",
      "epoch: 1000  loss: 0.42240384829989236  params: [1.43050523 1.35781415]  gradients: [-6.920222007232155e-05, -5.535551921561434e-05]\n",
      "epoch: 1100  loss: 0.4014625789268135  params: [1.51112952 1.41837672]  gradients: [-6.885401241466114e-05, -5.5159107309782776e-05]\n",
      "epoch: 1200  loss: 0.38612449859880654  params: [1.5908545  1.47852494]  gradients: [-6.853993506214291e-05, -5.4984966669524776e-05]\n",
      "epoch: 1300  loss: 0.37622578989217753  params: [1.66976234 1.53830134]  gradients: [-6.825649403656912e-05, -5.4830521792519954e-05]\n",
      "epoch: 1400  loss: 0.3716237476828062  params: [1.7479276  1.59774405]  gradients: [-6.800055389935702e-05, -5.4693489671208576e-05]\n",
      "epoch: 1500  loss: 0.3721940790294912  params: [1.82541788 1.65688722]  gradients: [-6.776930599935791e-05, -5.457185084747633e-05]\n",
      "epoch: 1600  loss: 0.3778285451466478  params: [1.90229447 1.71576146]  gradients: [-6.756023727754583e-05, -5.446382159875766e-05]\n",
      "epoch: 1700  loss: 0.38843290518096824  params: [1.97861294 1.77439415]  gradients: [-6.73711005856794e-05, -5.436782792627984e-05]\n",
      "epoch: 1800  loss: 0.4039251247530705  params: [2.05442367 1.83280983]  gradients: [-6.719988704762642e-05, -5.428248167875964e-05]\n"
     ]
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X, y, learning_rate = 0.0001, batch_size=16, num_epoch = 1900, tolerance = 0.00001, model=\"linear\" )\n",
    "# 선형 모델 생성 후 학습\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T) # 정규 방정식의 예측 결과\n",
    "y_hat_GD = new_param.dot(X.T) # 경사하강법의 예측 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 번 시도해본 결과, 1900에폭에서 최소 오차가 나온 것을 확인할 수 있다.\n",
    "\n",
    "이 이상 발생하면 과적합 등으로 오차가 다시 커지는 것을 확인할 수 있었다. 이는 학습률에 대한 문제로, 학습률을 수정하면 조금 더 정규방정식에 적합해질 것으로 보인다.\n",
    "\n",
    "실제 에폭을 달리해서 그래프를 확인해 보았는데, 빨간 선(GD) 가 점점 파란 선(NE)에 가까워지는 것을 확인할 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
