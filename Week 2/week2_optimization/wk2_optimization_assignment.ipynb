{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 15기 2주차 Optimization 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
    "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns) # 그냥 transForm도 가능하다.\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87727927, 0.17147902, 0.81885346])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z +=  X[i] * parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p =  \\frac{1}{1 + e^{-X_i\\theta }}$\n",
    "\n",
    "- sigmoid 함수라고도 부른다.\n",
    "- 분류 문제를 해결하기 위해 주로 사용한다(0~1의 범위를 갖기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X, parameters)\n",
    "    p = 1 / ( 1 + np.exp(-z))    # 로지스틱 함수\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7533548535671697"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수 == `MSE` 라 한다.\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
    "## $l(p) = -\\Sigma\\left \\{y_iX_i\\theta - log( 1 + e^{X_i\\theta})\\right \\} $\n",
    "\n",
    "아래 내용에선 $ -\\Sigma\\left \\{y_i log( \\frac {p_i}{1 -p_i}) + log( 1 - p_i)\\right \\}$ 로 작성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X, parameters)\n",
    "    loss = (y * np.log((p) / (1-p))) + np.log(1-p) # 오차값에 로그를 씌운 값입니다.\n",
    "    return -np.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X, parameters)\n",
    "    loss = (y - y_hat) ** 2 # Squared error\n",
    "    return 0.5 * np.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X, y, parameters)\n",
    "    loss = (loss / n) #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3543687197029732"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)= -\\Sigma(y_i - \\theta^{T}X_i)X_{ij} = \\Sigma(\\theta^{T}X_i - y_i )X_{ij}$\n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= -\\Sigma(y_i - p_i)x_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X, parameters)\n",
    "        gradient = np.sum(y_hat - y) * X[j]\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = np.sum(p-y) * X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5793058718649838"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 2, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))] # 0으로 초기화했다.\n",
    "    \n",
    "    for i in range(len(X_set)): # == X_set.shape[0] 과 같다.\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X, y, parameters, j, model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59.134440439184445, 3.4432282373288188, 39.902636991959355]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch => range 문을 위해 +1을 해줌.\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "### 설명\n",
    "`batch_idx` 는 `X_train` 과 `batch_size` 를 받는다.\n",
    "\n",
    "`X_train` 은 학습에 사용되는 데이터 행렬이며, `batch_size` 는 미니배치의 크기이다.\n",
    "\n",
    "함수 내부에서 `N`, `nb` 두개의 변수를 만드는데, 각각 데이터의 개수와, 배치로 나눴을 때 배치 덩어리의 개수이다.\n",
    "\n",
    "총 데이터의 인덱스 (`N`의 길이)를 `idx` 배열에 저장한다.\n",
    "\n",
    "그 후에 그것을 `batch_size` 단위로 잘라서 `idx_list` 에 저장한다.\n",
    "\n",
    "즉 위의 함수는, 전체 데이터로 미니배치 데이터를 만들고, 배치 안에 들어있는 데이터를 인덱스로 표시하여 반환해주는 함수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= (learning_rate / n) # 평균을 구하고 학습률을 곱함\n",
    "    \n",
    "    parameters -= gradients[i]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87461909, 0.16881884, 0.81619328])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train)) # 갱신된 파라미터 반환함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: 전체 데이터를 한번 돌 때 1에포크 증가\n",
    "- num_epoch: 전체 에포크를 몇 번 반복할 것인지\n",
    "<br>\n",
    "\n",
    "BGD: \"Batch Gradient Descent\" : 학습 한 번에 모든 데이터셋 전체를 순회하며 기울기를 구한다.  \n",
    "SGD: \"Stochasitc Gradient Descent\" : 랜덤하게 데이터 몇 개(또는 한 개)를 뽑아서 기울기를 구한다.  \n",
    "MGD: \"Mini batch Gradient Descent\" : 미니배치를 사용해(데이터의 일부를) 기울기를 구한다. 미니배치 전체를 돌면 학습 한 번에 모든 데이터셋을 돌게 된다. BGD와 SGD의 장점을 합친 방법으로 볼 수 있다.  \n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요  \n",
    "batch_size=1 -> SGD  \n",
    "batch_size=k -> MGD  \n",
    "batch_size=whole -> BGD  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0]) # 데이터의 feature 개수이다.\n",
    "    # 성능의 공정한 평가를 위해서 시드 고정\n",
    "    np.random.seed(0)\n",
    "    parameters = np.random.rand(N) # 랜덤한 계수로 초기화했다.\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i # 모델에 따라 손실 함수가 달라져야 한다.\n",
    "    loss = 999 # 임의의 loss(엄청 큰 값)\n",
    "    batch_idx_list = batch_idx(X_train, batch_size) # 모델을 1배치에 해당하는 X행렬 원소의 인덱스를 넣어 놓은 행렬 생성\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, loss_function) # 배치의 그래디언트를 구한다.\n",
    "            parameters = step(parameters, gradients, learning_rate, batch_size) # 구한 그래디언트를 미분하여 행렬을 갱신한다.\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, batch_size) # 손실 함수를 이용하여 손실값을 구한다.\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                print(f\"Early Stopped at loss: {new_loss}\") # early stop 표기\n",
    "                break\n",
    "            loss = new_loss\n",
    "            \n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 하이퍼파라미터의 모델 간 성능 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.9677272641313283  params: [0.53350539 0.69988125 0.58745526]  gradients: [0.015905073327806236, 0.005180187730894569, 0.015308116334578011]\n",
      "epoch: 100  loss: 0.6472123775798854  params: [-0.21665658 -0.05028072 -0.16270671]  gradients: [0.00835828694439114, -0.008071488942068621, 0.0022873455227756688]\n",
      "Early Stopped at loss: 0.6460791890728227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.25815299, -0.09177713, -0.20420312])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, learning_rate = 0.05, batch_size=len(X_train), tolerance = 0.00001, model=\"logistic\")\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.9677272641313283  params: [0.53350539 0.69988125 0.58745526]  gradients: [0.015905073327806236, 0.005180187730894569, 0.015308116334578011]\n",
      "epoch: 100  loss: 0.6472123775798854  params: [-0.21665658 -0.05028072 -0.16270671]  gradients: [0.00835828694439114, -0.008071488942068621, 0.0022873455227756688]\n",
      "Early Stopped at loss: 0.6460791890728227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.25815299, -0.09177713, -0.20420312])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate = 0.05, batch_size=len(X_train), tolerance = 0.00001, model=\"logistic\")\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.5289283926833094  params: [0.41065559 0.57703145 0.46460546]  gradients: [0.01135775202378877, 0.010390592477806016, 0.01319902854674179]\n",
      "epoch: 100  loss: 0.1784799735128428  params: [-0.33862885 -0.17225299 -0.28467898]  gradients: [0.003122524078108122, 0.0024844512496437978, 0.004971729605929294]\n",
      "epoch: 200  loss: 0.17847996929029325  params: [-0.33862887 -0.17225301 -0.284679  ]  gradients: [0.003122523862029447, 0.002484451042755155, 0.00497172939131419]\n",
      "epoch: 300  loss: 0.17847996929029308  params: [-0.33862887 -0.17225301 -0.284679  ]  gradients: [0.0031225238620294377, 0.0024844510427551453, 0.00497172939131418]\n",
      "epoch: 400  loss: 0.17847996929029308  params: [-0.33862887 -0.17225301 -0.284679  ]  gradients: [0.0031225238620294377, 0.0024844510427551453, 0.00497172939131418]\n",
      "epoch: 500  loss: 0.17847996929029308  params: [-0.33862887 -0.17225301 -0.284679  ]  gradients: [0.0031225238620294377, 0.0024844510427551453, 0.00497172939131418]\n",
      "epoch: 600  loss: 0.17847996929029308  params: [-0.33862887 -0.17225301 -0.284679  ]  gradients: [0.0031225238620294377, 0.0024844510427551453, 0.00497172939131418]\n",
      "epoch: 700  loss: 0.17847996929029308  params: [-0.33862887 -0.17225301 -0.284679  ]  gradients: [0.0031225238620294377, 0.0024844510427551453, 0.00497172939131418]\n",
      "epoch: 800  loss: 0.17847996929029308  params: [-0.33862887 -0.17225301 -0.284679  ]  gradients: [0.0031225238620294377, 0.0024844510427551453, 0.00497172939131418]\n",
      "epoch: 900  loss: 0.17847996929029308  params: [-0.33862887 -0.17225301 -0.284679  ]  gradients: [0.0031225238620294377, 0.0024844510427551453, 0.00497172939131418]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.33862887, -0.17225301, -0.284679  ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.05, batch_size=16, num_epoch = 1000, tolerance = 0.00001, model=\"logistic\")\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 하이퍼파라미터로 비교했을 때 MGD가 가장 적은 loss 를 보였다.\n",
    "\n",
    "이후 MGD의 하이퍼파라미터를 조정해가며 학습을 비교해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.7801054684429901  params: [0.47816848 0.64454434 0.53211835]  gradients: [0.011799936842640017, 0.008623766736598808, 0.01765607925473053]\n",
      "epoch: 100  loss: 0.4126957459228419  params: [-0.33559063 -0.16921476 -0.28164075]  gradients: [0.0026848342358628714, -0.00218887136654188, 0.004219010159612238]\n",
      "epoch: 200  loss: 0.4126850786311187  params: [-0.33570808 -0.16933221 -0.2817582 ]  gradients: [0.0026835397229278165, -0.0021904714670937936, 0.004217039337701465]\n",
      "epoch: 300  loss: 0.4126850766367315  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.002683539480792918, -0.0021904717663850824, 0.00421703896906922]\n",
      "epoch: 400  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.002683539480747618, -0.0021904717664410745, 0.0042170389690002535]\n",
      "epoch: 500  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.0026835394807476105, -0.0021904717664410823, 0.004217038969000245]\n",
      "epoch: 600  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.0026835394807476105, -0.0021904717664410823, 0.004217038969000245]\n",
      "epoch: 700  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.0026835394807476105, -0.0021904717664410823, 0.004217038969000245]\n",
      "epoch: 800  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.0026835394807476105, -0.0021904717664410823, 0.004217038969000245]\n",
      "epoch: 900  loss: 0.41268507663635834  params: [-0.3357081  -0.16933224 -0.28175823]  gradients: [0.0026835394807476105, -0.0021904717664410823, 0.004217038969000245]\n"
     ]
    }
   ],
   "source": [
    "# 배치 사이즈 2배\n",
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.05, batch_size=32, num_epoch = 1000, tolerance = 0.00001, model=\"logistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 사이즈가 증가하니까, 오차가 증가하는 것으로 보였다.\n",
    "\n",
    "한 배치에 처리해야할 양의 데이터가 많아서 오차가 증가하는 것으로 직관적으로 이해할 수 있을 듯 하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.6072862613061332  params: [0.5203156  0.68669146 0.57426547]  gradients: [0.0024076645454719587, 0.0021868734916866725, 0.002754347384836322]\n",
      "epoch: 100  loss: 0.184456021427748  params: [-0.31110378 -0.14472792 -0.25715391]  gradients: [0.0006750126197253353, 0.0005454691017361947, 0.0010447287276680584]\n",
      "epoch: 200  loss: 0.17975566431513915  params: [-0.33256478 -0.16618892 -0.27861491]  gradients: [0.0006289253501337033, 0.0005011245055338196, 0.000998738229791742]\n",
      "epoch: 300  loss: 0.17960719236979922  params: [-0.333265   -0.16688913 -0.27931512]  gradients: [0.0006274407621498515, 0.0004997020919945801, 0.0009972627392148092]\n",
      "epoch: 400  loss: 0.1796022841958635  params: [-0.33328817 -0.16691231 -0.2793383 ]  gradients: [0.0006273916534486687, 0.000499655046607651, 0.0009972139380341834]\n",
      "epoch: 500  loss: 0.17960212169869913  params: [-0.33328894 -0.16691307 -0.27933906]  gradients: [0.0006273900275500772, 0.0004996534890288767, 0.000997212322324253]\n",
      "epoch: 600  loss: 0.17960211631856435  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.000627389973718001, 0.000499653437458818, 0.0009972122688295224]\n",
      "epoch: 700  loss: 0.17960211614043262  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899719356663, 0.0004996534357513767, 0.0009972122670583567]\n",
      "epoch: 800  loss: 0.17960211613453494  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718766554, 0.0004996534356948453, 0.0009972122669997154]\n",
      "epoch: 900  loss: 0.17960211613433968  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718747018, 0.0004996534356929737, 0.0009972122669977743]\n",
      "epoch: 1000  loss: 0.179602116134333  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718746352, 0.0004996534356929102, 0.000997212266997708]\n",
      "epoch: 1100  loss: 0.17960211613433225  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718746292, 0.0004996534356929054, 0.0009972122669977023]\n",
      "epoch: 1200  loss: 0.1796021161343321  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718746284, 0.0004996534356929053, 0.000997212266997702]\n",
      "epoch: 1300  loss: 0.17960211613433202  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718746284, 0.0004996534356929057, 0.0009972122669977017]\n",
      "epoch: 1400  loss: 0.1796021161343319  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718746279, 0.0004996534356929055, 0.0009972122669977017]\n",
      "epoch: 1500  loss: 0.1796021161343319  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718746279, 0.0004996534356929055, 0.0009972122669977017]\n",
      "epoch: 1600  loss: 0.1796021161343319  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718746279, 0.0004996534356929055, 0.0009972122669977017]\n",
      "epoch: 1700  loss: 0.1796021161343319  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718746279, 0.0004996534356929055, 0.0009972122669977017]\n",
      "epoch: 1800  loss: 0.1796021161343319  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718746279, 0.0004996534356929055, 0.0009972122669977017]\n",
      "epoch: 1900  loss: 0.1796021161343319  params: [-0.33328896 -0.1669131  -0.27933909]  gradients: [0.0006273899718746279, 0.0004996534356929055, 0.0009972122669977017]\n"
     ]
    }
   ],
   "source": [
    "# 학습률 0.01 에폭 수 증가\n",
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.01, batch_size=16, num_epoch = 2000, tolerance = 0.00001, model=\"logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.6177397323794496  params: [0.53451305 0.70088891 0.58846292]  gradients: [0.001211817169525006, 0.0010995921315023873, 0.0013836537287719175]\n",
      "epoch: 100  loss: 0.21373968175559577  params: [-0.19985939 -0.03348353 -0.14590952]  gradients: [0.0004642313163376731, 0.00039727377336907393, 0.0006513982554306244]\n",
      "epoch: 200  loss: 0.18470695661697462  params: [-0.30999446 -0.1436186  -0.25604459]  gradients: [0.00033814371005279917, 0.00027335034644240705, 0.0005230028962854754]\n",
      "epoch: 300  loss: 0.1806122976745067  params: [-0.32855262 -0.16217676 -0.27460275]  gradients: [0.0003181918234717324, 0.0002541381498189225, 0.0005030783105530107]\n",
      "epoch: 400  loss: 0.179899813064738  params: [-0.33188633 -0.16551046 -0.27793645]  gradients: [0.00031465241807676397, 0.0002507440805193676, 0.0004995577260740486]\n",
      "epoch: 500  loss: 0.1797710535004266  params: [-0.33249228 -0.16611642 -0.27854241]  gradients: [0.00031401055404151567, 0.00025012904339920017, 0.0004989197414842908]\n",
      "epoch: 600  loss: 0.17974762364812344  params: [-0.33260266 -0.1662268  -0.27865279]  gradients: [0.0003138936828671293, 0.00025001707252670437, 0.0004988035921796224]\n",
      "epoch: 700  loss: 0.17974335487180265  params: [-0.33262278 -0.16624691 -0.2786729 ]  gradients: [0.00031387238720111593, 0.0002499966702878365, 0.0004987824285637576]\n",
      "epoch: 800  loss: 0.17974257694929446  params: [-0.33262644 -0.16625058 -0.27867657]  gradients: [0.0003138685062937925, 0.0002499929522152998, 0.0004987785717382582]\n",
      "epoch: 900  loss: 0.1797424351783169  params: [-0.33262711 -0.16625125 -0.27867724]  gradients: [0.0003138677990226672, 0.0002499922746203703, 0.0004987778688564627]\n",
      "epoch: 1000  loss: 0.17974240934134442  params: [-0.33262723 -0.16625137 -0.27867736]  gradients: [0.000313867670126347, 0.0002499921511323946, 0.000498777740760093]\n",
      "epoch: 1100  loss: 0.17974240463269325  params: [-0.33262725 -0.16625139 -0.27867738]  gradients: [0.0003138676466356737, 0.0002499921286273642, 0.0004987777174152068]\n",
      "epoch: 1200  loss: 0.17974240377456646  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764235462246, 0.00024999212452594114, 0.0004987777131607246]\n",
      "epoch: 1300  loss: 0.17974240361817734  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764157442337, 0.0002499921237784783, 0.0004987777123853675]\n",
      "epoch: 1400  loss: 0.17974240358967633  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764143223654, 0.00024999212364225744, 0.0004987777122440633]\n",
      "epoch: 1500  loss: 0.17974240358448212  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.0003138676414063235, 0.00024999212361743176, 0.0004987777122183111]\n",
      "epoch: 1600  loss: 0.17974240358353558  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140160117, 0.0002499921236129075, 0.0004987777122136179]\n",
      "epoch: 1700  loss: 0.17974240358336302  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140074036, 0.0002499921236120828, 0.0004987777122127625]\n",
      "epoch: 1800  loss: 0.1797424035833315  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.000313867641400583, 0.0002499921236119321, 0.0004987777122126061]\n",
      "epoch: 1900  loss: 0.17974240358332585  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140055507, 0.00024999212361190537, 0.0004987777122125784]\n",
      "epoch: 2000  loss: 0.17974240358332533  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140055204, 0.0002499921236119021, 0.0004987777122125753]\n",
      "epoch: 2100  loss: 0.17974240358332516  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.0003138676414005509, 0.00024999212361190087, 0.0004987777122125741]\n",
      "epoch: 2200  loss: 0.1797424035833251  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140055084, 0.00024999212361190065, 0.000498777712212574]\n",
      "epoch: 2300  loss: 0.1797424035833251  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140055084, 0.00024999212361190065, 0.000498777712212574]\n",
      "epoch: 2400  loss: 0.1797424035833251  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140055084, 0.00024999212361190065, 0.000498777712212574]\n",
      "epoch: 2500  loss: 0.1797424035833251  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140055084, 0.00024999212361190065, 0.000498777712212574]\n",
      "epoch: 2600  loss: 0.1797424035833251  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140055084, 0.00024999212361190065, 0.000498777712212574]\n",
      "epoch: 2700  loss: 0.1797424035833251  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140055084, 0.00024999212361190065, 0.000498777712212574]\n",
      "epoch: 2800  loss: 0.1797424035833251  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140055084, 0.00024999212361190065, 0.000498777712212574]\n",
      "epoch: 2900  loss: 0.1797424035833251  params: [-0.33262726 -0.1662514  -0.27867739]  gradients: [0.00031386764140055084, 0.00024999212361190065, 0.000498777712212574]\n"
     ]
    }
   ],
   "source": [
    "# 학습률 0.005 # 에폭 수 증가 3000\n",
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.005, batch_size=16, num_epoch = 3000, tolerance = 0.00001, model=\"logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.62812061138106  params: [0.5485265  0.71490236 0.60247637]  gradients: [2.439047938033802e-05, 2.2109722678612358e-05, 2.7797142149859685e-05]\n",
      "epoch: 100  loss: 0.6071135720468446  params: [0.52008031 0.68645618 0.57403019]  gradients: [2.403925819393069e-05, 2.18397721636968e-05, 2.7512979620481583e-05]\n",
      "epoch: 200  loss: 0.5867459113578799  params: [0.4921503  0.65852617 0.54610018]  gradients: [2.3677173396233126e-05, 2.1556540295239516e-05, 2.7214713576050615e-05]\n",
      "epoch: 300  loss: 0.567031314352197  params: [0.46475435 0.63113021 0.51870422]  gradients: [2.330479763650501e-05, 2.1260203700969453e-05, 2.690258480496404e-05]\n",
      "epoch: 400  loss: 0.5479816350229141  params: [0.43790979 0.60428565 0.49185966]  gradients: [2.292281778422369e-05, 2.095107681830611e-05, 2.6576982988232825e-05]\n",
      "epoch: 500  loss: 0.5296066657436156  params: [0.41163322 0.57800908 0.46558309]  gradients: [2.2532033979728926e-05, 2.0629619937088764e-05, 2.6238453304473638e-05]\n",
      "epoch: 600  loss: 0.5119139253003918  params: [0.38594031 0.55231617 0.43989018]  gradients: [2.2133355767327168e-05, 2.0296443905126426e-05, 2.5887699327712423e-05]\n",
      "epoch: 700  loss: 0.4949084738903603  params: [0.36084557 0.52722143 0.41479544]  gradients: [2.1727795093718326e-05, 1.995231083109825e-05, 2.5525581626385763e-05]\n",
      "epoch: 800  loss: 0.47859276327116257  params: [0.33636219 0.50273805 0.39031206]  gradients: [2.1316456093139056e-05, 1.959813026677443e-05, 2.5153111641983196e-05]\n",
      "epoch: 900  loss: 0.4629665295266453  params: [0.31250184 0.47887771 0.36645172]  gradients: [2.0900521748044826e-05, 1.923495056607859e-05, 2.4771440656970207e-05]\n",
      "epoch: 1000  loss: 0.448026734627498  params: [0.2892745  0.45565036 0.34322437]  gradients: [2.0481237699232167e-05, 1.8863945390594777e-05, 2.438184394031854e-05]\n",
      "epoch: 1100  loss: 0.43376756114771475  params: [0.26668827 0.43306413 0.32063814]  gradients: [2.005989366632976e-05, 1.8486395638330517e-05, 2.3985700462724307e-05]\n",
      "epoch: 1200  loss: 0.42018046224765365  params: [0.24474931 0.41112517 0.29869918]  gradients: [1.9637803110001247e-05, 1.8103667385483404e-05, 2.358446887327986e-05]\n",
      "epoch: 1300  loss: 0.4072542665104718  params: [0.2234617  0.38983756 0.27741157]  gradients: [1.9216281902067848e-05, 1.771718671598687e-05, 2.3179660692893383e-05]\n",
      "epoch: 1400  loss: 0.3949753346224478  params: [0.2028274  0.36920326 0.25677727]  gradients: [1.8796626852714522e-05, 1.732841253796582e-05, 2.277281187695193e-05]\n",
      "epoch: 1500  loss: 0.38332776243993677  params: [0.18284622 0.34922208 0.23679609]  gradients: [1.8380094964036648e-05, 1.6938808623461145e-05, 2.236545400736432e-05]\n",
      "epoch: 1600  loss: 0.3722936228971984  params: [0.16351585 0.32989172 0.21746572]  gradients: [1.7967884232918662e-05, 1.6549816142307585e-05, 2.195908637993343e-05]\n",
      "epoch: 1700  loss: 0.3618532376522501  params: [0.14483194 0.3112078  0.19878181]  gradients: [1.756111671832905e-05, 1.6162827890777494e-05, 2.1555150157787764e-05]\n",
      "epoch: 1800  loss: 0.351985468453463  params: [0.12678813 0.29316399 0.180738  ]  gradients: [1.7160824430770832e-05, 1.5779165251988545e-05, 2.1155005578877346e-05]\n",
      "epoch: 1900  loss: 0.34266801797602275  params: [0.10937624 0.2757521  0.16332611]  gradients: [1.6767938412463114e-05, 1.5400058690678197e-05, 2.075991295892707e-05]\n",
      "epoch: 2000  loss: 0.33387773028979184  params: [0.09258638 0.25896224 0.14653625]  gradients: [1.6383281175983435e-05, 1.502663230917879e-05, 2.0371017950192838e-05]\n",
      "epoch: 2100  loss: 0.3255908820818376  params: [0.07640712 0.24278298 0.13035699]  gradients: [1.6007562476132244e-05, 1.465989270549095e-05, 1.998934123129184e-05]\n",
      "epoch: 2200  loss: 0.3177834571284573  params: [0.06082568 0.22720154 0.11477555]  gradients: [1.5641378221026663e-05, 1.4300722106547834e-05, 1.9615772541227497e-05]\n",
      "epoch: 2300  loss: 0.3104313981334718  params: [0.04582813 0.21220399 0.099778  ]  gradients: [1.5285212195262924e-05, 1.394987552198701e-05, 1.925106875180231e-05]\n",
      "epoch: 2400  loss: 0.3035108317641054  params: [0.03139953 0.1977754  0.08534941]  gradients: [1.4939440176136626e-05, 1.3607981489662902e-05, 1.8895855509002905e-05]\n",
      "epoch: 2500  loss: 0.2969982643845591  params: [0.0175242  0.18390006 0.07147407]  gradients: [1.4604335973792188e-05, 1.3275545869001299e-05, 1.8550631869570644e-05]\n",
      "epoch: 2600  loss: 0.29087074750269865  params: [0.0041858  0.17056167 0.05813568]  gradients: [1.4280078913927244e-05, 1.2952958080123792e-05, 1.821577731083841e-05]\n",
      "epoch: 2700  loss: 0.2851060132349598  params: [-0.00863241  0.15774345  0.04531746]  gradients: [1.3966762300587726e-05, 1.2640499178146947e-05, 1.789156049206968e-05]\n",
      "epoch: 2800  loss: 0.27968258112118655  params: [-0.0209475   0.14542836  0.03300237]  gradients: [1.366440243854047e-05, 1.233835118276168e-05, 1.7578149183237947e-05]\n",
      "Early Stopped at loss: 0.6729297167952434\n"
     ]
    }
   ],
   "source": [
    "# 학습률 0.0001 # 에폭 수 증가 3000\n",
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.0001, batch_size=16, num_epoch = 3000, tolerance = 0.00001, model=\"logistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 많이 낮추지 않는 이상 드라마틱한 변화는 일어나지 않는 것을 확인할 수 있었다.\n",
    "\n",
    "학습률이 낮아지면 낮아질수록 loss 가 줄어드는 속도가 변화하는 것을 확인할 수 있었다.\n",
    "\n",
    "이 경우, 시간에 따라서 학습률을 변화시키는  `RMSProp` 이나 `Adam` 과 같은 옵티마이저가 필요할 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd) # 학습한 bgd를 가지고 예측 결과 산출\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35,  5],\n",
       "       [ 9,  1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn) # 구한 값으로 비교\n",
    "print(\"accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)]) # 편향 1 추가\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35767412, 2.75736157])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.998870471497711  params: [0.54950364 0.7158795 ]  gradients: [-7.562012280425182e-05, -5.9357993741702534e-05]\n",
      "epoch: 100  loss: 0.9222304676775057  params: [0.61800199 0.78437785]  gradients: [-7.484326619752282e-05, -5.8835078094193e-05]\n",
      "epoch: 200  loss: 0.8514943882377101  params: [0.68552873 0.8519046 ]  gradients: [-7.412215344387495e-05, -5.835490430851589e-05]\n",
      "epoch: 300  loss: 0.7863893330606967  params: [0.7521544  0.91853026]  gradients: [-7.345428754672587e-05, -5.791529591539536e-05]\n",
      "epoch: 400  loss: 0.7266675518809248  params: [0.81794639 0.98432225]  gradients: [-7.283684907680004e-05, -5.7513827845847154e-05]\n",
      "epoch: 500  loss: 0.6721050261609527  params: [0.88296859 1.04934445]  gradients: [-7.226683719648956e-05, -5.714794502126668e-05]\n",
      "epoch: 600  loss: 0.6224998757341496  params: [0.94728119 1.11365705]  gradients: [-7.174117968450145e-05, -5.681505420461459e-05]\n",
      "epoch: 700  loss: 0.5776706830153838  params: [1.01094057 1.17731643]  gradients: [-7.125681594297341e-05, -5.651259260550821e-05]\n",
      "epoch: 800  loss: 0.5374548038271285  params: [1.07399933 1.24037519]  gradients: [-7.081075736931349e-05, -5.623807707530603e-05]\n",
      "epoch: 900  loss: 0.5017067141148727  params: [1.13650634 1.30288221]  gradients: [-7.040012935967169e-05, -5.5989137618608656e-05]\n",
      "epoch: 1000  loss: 0.47029642599511806  params: [1.19850691 1.36488278]  gradients: [-7.002219880866727e-05, -5.5763538587274674e-05]\n",
      "epoch: 1100  loss: 0.44310799431540854  params: [1.2600429  1.42641877]  gradients: [-6.967439043884797e-05, -5.555919045165673e-05]\n",
      "epoch: 1200  loss: 0.4200381256840506  params: [1.32115293 1.48752879]  gradients: [-6.935429473408707e-05, -5.537415454990016e-05]\n",
      "epoch: 1300  loss: 0.4009948951927676  params: [1.38187255 1.54824841]  gradients: [-6.90596697220933e-05, -5.520664275082097e-05]\n",
      "epoch: 1400  loss: 0.38589657129001154  params: [1.44223447 1.60861033]  gradients: [-6.87884383817555e-05, -5.505501355443693e-05]\n",
      "epoch: 1500  loss: 0.3746705460141951  params: [1.50226872 1.66864459]  gradients: [-6.853868305198371e-05, -5.4917765805698094e-05]\n",
      "epoch: 1600  loss: 0.36725236568964276  params: [1.56200288 1.72837875]  gradients: [-6.83086378899025e-05, -5.4793530910820226e-05]\n",
      "epoch: 1700  loss: 0.3635848559223614  params: [1.62146222 1.78783808]  gradients: [-6.809668016175863e-05, -5.4681064216299427e-05]\n",
      "epoch: 1800  loss: 0.3636173340725567  params: [1.68066991 1.84704577]  gradients: [-6.790132094121975e-05, -5.457923603046604e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.73905847, 1.90543433])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X, y, learning_rate = 0.0001, batch_size=16, num_epoch = 1900, tolerance = 0.00001, model=\"linear\" )\n",
    "# 선형 모델 생성 후 학습\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T) # 정규 방정식의 예측 결과\n",
    "y_hat_GD = new_param.dot(X.T) # 경사하강법의 예측 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 번 시도해본 결과, 1900에폭에서 최소 오차가 나온 것을 확인할 수 있다.\n",
    "\n",
    "이 이상 발생하면 과적합 등으로 오차가 다시 커지는 것을 확인할 수 있었다. 이는 학습률에 대한 문제로, 학습률을 수정하면 조금 더 정규방정식에 적합해질 것으로 보인다.\n",
    "\n",
    "실제 에폭을 달리해서 그래프를 확인해 보았는데, 빨간 선(GD) 가 점점 파란 선(NE)에 가까워지는 것을 확인할 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
